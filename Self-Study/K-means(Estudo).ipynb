{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bIjtaenPLFxw8msPbdrIo82BFqnRt_nw","timestamp":1723116280197},{"file_id":"1zfbwRNvETu5Nrt1jMuJpaZSvjirsB5P5","timestamp":1674511371682}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Introdução ao K-means\n","[link text](https://pt.wikipedia.org/wiki/K-means)\n","![picture](https://drive.google.com/uc?export=view&id=160HEkR11QhXN8id6pJLIvN-6lKQiUwZo)"],"metadata":{"id":"oLVcI2B6iSay"}},{"cell_type":"markdown","source":["## K-Means Versão spark\n","  Neste exemplo consideramos que geramos 1000 pontos randomicos, no intervalo\n","  [1,5000] e queremos clusterizar em k grupos."],"metadata":{"id":"JglQBkEgxWFq"}},{"cell_type":"markdown","source":["Iniciamos instalando o pyspark"],"metadata":{"id":"ItQBlXjrxwqz"}},{"cell_type":"code","source":["pip install pyspark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uiGEOlx23cED","executionInfo":{"status":"ok","timestamp":1675188925764,"user_tz":180,"elapsed":45905,"user":{"displayName":"Fabio Porto","userId":"01823619445150565979"}},"outputId":"8dbe6cae-c7f2-4dd9-aecc-58e7e42debbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=3e052692385662b9be59a8637b88da0bcd184851fd1aa2dfe8c4b4eeec3a922f\n","  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}]},{"cell_type":"markdown","source":["## Importamos pyspark - API python para Spark\n","##            radom - para geração pesudo-aleatório de inteiros"],"metadata":{"id":"Thz_F268x52m"}},{"cell_type":"code","source":["import pyspark as ps\n","import random"],"metadata":{"id":"Rk7rU1Ar3xa_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importamos o SparkContext para definição do espaço RDD\n","- Deve existir apenas um SparkContext por job spark\n","- Note que estamos usando a API RDD.. Mais adiante, usaremos Dataframe"],"metadata":{"id":"N4DI7mN0yN0B"}},{"cell_type":"code","source":["from pyspark.context import SparkContext\n","sc= SparkContext(appName=\"K-means\", master=\"local[4]\")"],"metadata":{"id":"YHMm1Dw7ACHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Iniciamos um acumulador - Fica armazenado no Driver e pode ser acumulado\n","## a partir dos nós trabalhadores"],"metadata":{"id":"fA6nXqqjyebn"}},{"cell_type":"code","source":["# creates and initializes an accumulator to zero\n","acum=sc.accumulator(0)"],"metadata":{"id":"M1guqjQ6wqst"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Criamos uma classe para iniciar as duas listas:\n","\n","*   centroids - conjunto de centroids dos clustes. Inicialmente vazia\n","*   points - lista de pontos a clusterizar\n","\n","\n","  -"],"metadata":{"id":"C_X8mtlzyzPy"}},{"cell_type":"code","source":["class K_means_exemplo_MR:\n","  def __init__(self, k):\n","    self.centroids=random.sample(range(1,1000),k)\n","    self.points=sc.parallelize(random.sample(range(1,5000),1000),2)\n","\n"],"metadata":{"id":"aJ8tmRlCchSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Função findCluster\n","  input= p-> ponto sendo avaliado; centroids-> lista de centroids\n","  Objetivo:\n","    c=varrer a lista e encontrar o centroid mais proximo ao ponto \"p\""],"metadata":{"id":"HtJf8ZHhzMMq"}},{"cell_type":"code","source":["def findCluster(p,centroids):\n","    min=999999\n","    for c in centroids:\n","      newdist=abs(p-c)\n","      if  newdist < min:\n","        centroid=c\n","        min = newdist\n","    return [centroid,p]"],"metadata":{"id":"qO5lzEWTUV-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Função K_Means - função principal\n","  obtém a lista de centroids  randômicos - iniciais (centroids_updt)\n","  inicializa a lista de centroids\n","  realiza um loop até que o processo convirja e centroids == centroids_updt\n","     a função map chama a findCluster-> forma o par (centroid, ponto)\n","     utiliza a função groupByKey para gerar (K,[V])\n","     calcula a media com a soma/|V| dos valores em V\n","     transforma a tupla em  RDD com centroids\n","     incrementa no acumulador o numero de iterações"],"metadata":{"id":"tQJWHGbf04sd"}},{"cell_type":"code","source":["def k_means(k):\n","\n","  k_means = K_means_exemplo_MR(k)\n","  centroids = list([])\n","  centroids_updt = k_means.centroids\n","  print(\"centroids\"+str((centroids)))\n","  print(\"centroid_upd\"+str(centroids_updt))\n","  points=k_means.points\n","  print(str(points.count()))\n","  iter=0\n","  while (sorted(centroids) != sorted(centroids_updt)):\n","    centroids=centroids_updt\n","    keysValues=points.map(lambda j:findCluster(j,centroids))\n","    #MapValues- Passa cada valor do par (key, Values) a função, sem mudar a chave\n","    reducedKeys=keysValues.groupByKey().mapValues(lambda x: sum(x) / len(x))\n","    newKeys=reducedKeys.map(lambda x:x[1])\n","    centroids_updt=newKeys.collect()\n","    acum.add(1)\n","\n","  return centroids_updt"],"metadata":{"id":"M-w8-SyBSu6l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Invoca o process"],"metadata":{"id":"M_uz2S9H21Ut"}},{"cell_type":"code","source":["k=3\n","groups = k_means(k)\n","print(str(groups)+ \"Number of iterations: \"+str(acum))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGs3ek9liyqK","executionInfo":{"status":"ok","timestamp":1675189767295,"user_tz":180,"elapsed":5590,"user":{"displayName":"Fabio Porto","userId":"01823619445150565979"}},"outputId":"3ab9e2bb-51ad-4985-ed51-0cb5ba9ca860"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["centroids[]\n","centroid_upd[784, 698, 849]\n","1000\n","[2514.5324675324673, 4165.158959537573, 839.9190751445087]Number of iterations: 33\n"]}]}]}